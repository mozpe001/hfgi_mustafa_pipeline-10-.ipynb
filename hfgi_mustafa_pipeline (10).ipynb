{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "hfgi_mustafa_pipeline.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Setup repository"
      ],
      "metadata": {
        "id": "CQS2d46fnEQM"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "ZkOGUdDHmEjw",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9dad8f9d-5cf4-4058-cfda-6235c95ea725"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'HFGI'...\n",
            "remote: Enumerating objects: 231, done.\u001b[K\n",
            "remote: Counting objects: 100% (139/139), done.\u001b[K\n",
            "remote: Compressing objects: 100% (99/99), done.\u001b[K\n",
            "remote: Total 231 (delta 41), reused 129 (delta 33), pack-reused 92\u001b[K\n",
            "Receiving objects: 100% (231/231), 15.81 MiB | 31.68 MiB/s, done.\n",
            "Resolving deltas: 100% (45/45), done.\n",
            "--2022-05-01 23:26:10--  https://github.com/ninja-build/ninja/releases/download/v1.8.2/ninja-linux.zip\n",
            "Resolving github.com (github.com)... 192.30.255.112\n",
            "Connecting to github.com (github.com)|192.30.255.112|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://objects.githubusercontent.com/github-production-release-asset-2e65be/1335132/d2f252e2-9801-11e7-9fbf-bc7b4e4b5c83?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=AKIAIWNJYAX4CSVEH53A%2F20220501%2Fus-east-1%2Fs3%2Faws4_request&X-Amz-Date=20220501T232610Z&X-Amz-Expires=300&X-Amz-Signature=bd45d6712c30882adbcc4796b35c2209c243c61134dacc2e433c9089f242c12c&X-Amz-SignedHeaders=host&actor_id=0&key_id=0&repo_id=1335132&response-content-disposition=attachment%3B%20filename%3Dninja-linux.zip&response-content-type=application%2Foctet-stream [following]\n",
            "--2022-05-01 23:26:10--  https://objects.githubusercontent.com/github-production-release-asset-2e65be/1335132/d2f252e2-9801-11e7-9fbf-bc7b4e4b5c83?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=AKIAIWNJYAX4CSVEH53A%2F20220501%2Fus-east-1%2Fs3%2Faws4_request&X-Amz-Date=20220501T232610Z&X-Amz-Expires=300&X-Amz-Signature=bd45d6712c30882adbcc4796b35c2209c243c61134dacc2e433c9089f242c12c&X-Amz-SignedHeaders=host&actor_id=0&key_id=0&repo_id=1335132&response-content-disposition=attachment%3B%20filename%3Dninja-linux.zip&response-content-type=application%2Foctet-stream\n",
            "Resolving objects.githubusercontent.com (objects.githubusercontent.com)... 185.199.108.133, 185.199.111.133, 185.199.109.133, ...\n",
            "Connecting to objects.githubusercontent.com (objects.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 77854 (76K) [application/octet-stream]\n",
            "Saving to: ‘ninja-linux.zip’\n",
            "\n",
            "ninja-linux.zip     100%[===================>]  76.03K  --.-KB/s    in 0.009s  \n",
            "\n",
            "2022-05-01 23:26:10 (7.87 MB/s) - ‘ninja-linux.zip’ saved [77854/77854]\n",
            "\n",
            "Archive:  ninja-linux.zip\n",
            "  inflating: /usr/local/bin/ninja    \n",
            "update-alternatives: using /usr/local/bin/ninja to provide /usr/bin/ninja (ninja) in auto mode\n",
            "Cloning into 'stylegan2-distillation'...\n",
            "remote: Enumerating objects: 228, done.\u001b[K\n",
            "remote: Counting objects: 100% (12/12), done.\u001b[K\n",
            "remote: Compressing objects: 100% (12/12), done.\u001b[K\n",
            "remote: Total 228 (delta 5), reused 0 (delta 0), pack-reused 216\u001b[K\n",
            "Receiving objects: 100% (228/228), 45.38 MiB | 21.83 MiB/s, done.\n",
            "Resolving deltas: 100% (32/32), done.\n"
          ]
        }
      ],
      "source": [
        "!git clone https://github.com/Tengfei-Wang/HFGI.git $CODE_DIR\n",
        "!wget https://github.com/ninja-build/ninja/releases/download/v1.8.2/ninja-linux.zip\n",
        "!sudo unzip ninja-linux.zip -d /usr/local/bin/\n",
        "!sudo update-alternatives --install /usr/bin/ninja ninja /usr/local/bin/ninja 1 --force\n",
        "\n",
        "# for stylegan2 directions\n",
        "# !git clone https://github.com/genforce/interfacegan.git\n",
        "!git clone https://github.com/EvgenyKashin/stylegan2-distillation.git"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "# download the hfgi checkpoint\n",
        "path = {\"id\": \"19y6pxOiJWB0NoG3fAZO9Eab66zkN9XIL\", \"name\": \"ckpt.pt\"}\n",
        "file_id = path['id']\n",
        "FILE_NAME = path['name']\n",
        "\n",
        "save_path = 'HFGI/checkpoint/'\n",
        "if not os.path.isdir(save_path):\n",
        "    os.mkdir(save_path)\n",
        "\n",
        "url = r\"\"\"wget --load-cookies /tmp/cookies.txt \"https://docs.google.com/uc?export=download&confirm=$(wget --quiet --save-cookies /tmp/cookies.txt --keep-session-cookies --no-check-certificate 'https://docs.google.com/uc?export=download&id={FILE_ID}' -O- | sed -rn 's/.*confirm=([0-9A-Za-z_]+).*/\\1\\n/p')&id={FILE_ID}\" -O {SAVE_PATH}/{FILE_NAME} && rm -rf /tmp/cookies.txt\"\"\".format(FILE_ID=file_id, FILE_NAME='ckpt.pt', SAVE_PATH=save_path)\n",
        "\n",
        "!wget {url}\n",
        "\n",
        "# landmark model\n",
        "!wget https://raw.github.com/ageitgey/face_recognition_models/master/face_recognition_models/models/shape_predictor_68_face_landmarks.dat\n",
        "!mv shape_predictor_68_face_landmarks.dat $save_path"
      ],
      "metadata": {
        "id": "xUT_LHXSnjLT",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "dc696b14-eb9d-4bd9-91c3-fc12c41982ed"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2022-05-01 23:26:13--  http://wget/\n",
            "Resolving wget (wget)... failed: Name or service not known.\n",
            "wget: unable to resolve host address ‘wget’\n",
            "--2022-05-01 23:26:14--  https://docs.google.com/uc?export=download&confirm=t&id=19y6pxOiJWB0NoG3fAZO9Eab66zkN9XIL\n",
            "Resolving docs.google.com (docs.google.com)... 74.125.195.113, 74.125.195.138, 74.125.195.101, ...\n",
            "Connecting to docs.google.com (docs.google.com)|74.125.195.113|:443... connected.\n",
            "HTTP request sent, awaiting response... 303 See Other\n",
            "Location: https://doc-0c-2k-docs.googleusercontent.com/docs/securesc/ha0ro937gcuc7l7deffksulhg5h7mbp1/897ih8qlbh0cquvql4etv0vtgmd1t24t/1651447500000/17817459031648051118/*/19y6pxOiJWB0NoG3fAZO9Eab66zkN9XIL?e=download [following]\n",
            "Warning: wildcards not supported in HTTP.\n",
            "--2022-05-01 23:26:14--  https://doc-0c-2k-docs.googleusercontent.com/docs/securesc/ha0ro937gcuc7l7deffksulhg5h7mbp1/897ih8qlbh0cquvql4etv0vtgmd1t24t/1651447500000/17817459031648051118/*/19y6pxOiJWB0NoG3fAZO9Eab66zkN9XIL?e=download\n",
            "Resolving doc-0c-2k-docs.googleusercontent.com (doc-0c-2k-docs.googleusercontent.com)... 74.125.142.132, 2607:f8b0:400e:c08::84\n",
            "Connecting to doc-0c-2k-docs.googleusercontent.com (doc-0c-2k-docs.googleusercontent.com)|74.125.142.132|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 1342891936 (1.2G) [application/x-zip]\n",
            "Saving to: ‘HFGI/checkpoint//ckpt.pt’\n",
            "\n",
            "HFGI/checkpoint//ck 100%[===================>]   1.25G   131MB/s    in 8.4s    \n",
            "\n",
            "2022-05-01 23:26:23 (153 MB/s) - ‘HFGI/checkpoint//ckpt.pt’ saved [1342891936/1342891936]\n",
            "\n",
            "FINISHED --2022-05-01 23:26:23--\n",
            "Total wall clock time: 9.0s\n",
            "Downloaded: 1 files, 1.2G in 8.4s (153 MB/s)\n",
            "--2022-05-01 23:26:23--  https://raw.github.com/ageitgey/face_recognition_models/master/face_recognition_models/models/shape_predictor_68_face_landmarks.dat\n",
            "Resolving raw.github.com (raw.github.com)... 185.199.110.133, 185.199.111.133, 185.199.109.133, ...\n",
            "Connecting to raw.github.com (raw.github.com)|185.199.110.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 301 Moved Permanently\n",
            "Location: https://raw.githubusercontent.com/ageitgey/face_recognition_models/master/face_recognition_models/models/shape_predictor_68_face_landmarks.dat [following]\n",
            "--2022-05-01 23:26:23--  https://raw.githubusercontent.com/ageitgey/face_recognition_models/master/face_recognition_models/models/shape_predictor_68_face_landmarks.dat\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.111.133, 185.199.110.133, 185.199.108.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.111.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 99693937 (95M) [application/octet-stream]\n",
            "Saving to: ‘shape_predictor_68_face_landmarks.dat’\n",
            "\n",
            "shape_predictor_68_ 100%[===================>]  95.08M   125MB/s    in 0.8s    \n",
            "\n",
            "2022-05-01 23:26:24 (125 MB/s) - ‘shape_predictor_68_face_landmarks.dat’ saved [99693937/99693937]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Code"
      ],
      "metadata": {
        "id": "4WQsol0MkV4D"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from argparse import Namespace\n",
        "import time\n",
        "import os\n",
        "import sys\n",
        "import numpy as np\n",
        "from PIL import Image\n",
        "import torch\n",
        "import torchvision.transforms as transforms\n",
        "\n",
        "sys.path.append(\".\")\n",
        "sys.path.append(\"..\")\n",
        "\n",
        "sys.path.insert(1,'HFGI/')\n",
        "\n",
        "from utils.common import tensor2im\n",
        "from models.psp import pSp  # we use the pSp framework to load the e4e encoder.\n",
        "\n",
        "\n",
        "%load_ext autoreload\n",
        "%autoreload 2"
      ],
      "metadata": {
        "id": "Y9ahngnKjtWq"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## face alignment utils"
      ],
      "metadata": {
        "id": "I_v2_BBBkDiR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import scipy\n",
        "import scipy.ndimage\n",
        "import PIL\n",
        "from PIL import Image\n",
        "import numpy as np\n",
        "\n",
        "def get_landmark(img, predictor, detector):\n",
        "    \"\"\"get landmark with dlib\n",
        "    :return: np.array shape=(68, 2)\n",
        "    \"\"\"\n",
        "    # #detector = dlib.get_frontal_face_detector()\n",
        "\n",
        "    dets = detector(img, 1)\n",
        "\n",
        "    for k, d in enumerate(dets):\n",
        "        shape = predictor(img, d)\n",
        "\n",
        "    t = list(shape.parts())\n",
        "    a = []\n",
        "    for tt in t:\n",
        "        a.append([tt.x, tt.y])\n",
        "    lm = np.array(a)\n",
        "    return lm\n",
        "\n",
        "\n",
        "def align_face(img, predictor, detector):\n",
        "    \"\"\"\n",
        "    :param filepath: str\n",
        "    :return: PIL Image\n",
        "    \"\"\"\n",
        "\n",
        "    lm = get_landmark(np.array(img), predictor, detector)\n",
        "\n",
        "    lm_chin = lm[0: 17]  # left-right\n",
        "    lm_eyebrow_left = lm[17: 22]  # left-right\n",
        "    lm_eyebrow_right = lm[22: 27]  # left-right\n",
        "    lm_nose = lm[27: 31]  # top-down\n",
        "    lm_nostrils = lm[31: 36]  # top-down\n",
        "    lm_eye_left = lm[36: 42]  # left-clockwise\n",
        "    lm_eye_right = lm[42: 48]  # left-clockwise\n",
        "    lm_mouth_outer = lm[48: 60]  # left-clockwise\n",
        "    lm_mouth_inner = lm[60: 68]  # left-clockwise\n",
        "\n",
        "    # Calculate auxiliary vectors.\n",
        "    eye_left = np.mean(lm_eye_left, axis=0)\n",
        "    eye_right = np.mean(lm_eye_right, axis=0)\n",
        "    eye_avg = (eye_left + eye_right) * 0.5\n",
        "    eye_to_eye = eye_right - eye_left\n",
        "    mouth_left = lm_mouth_outer[0]\n",
        "    mouth_right = lm_mouth_outer[6]\n",
        "    mouth_avg = (mouth_left + mouth_right) * 0.5\n",
        "    eye_to_mouth = mouth_avg - eye_avg\n",
        "\n",
        "    # Choose oriented crop rectangle.\n",
        "    x = eye_to_eye - np.flipud(eye_to_mouth) * [-1, 1]\n",
        "    x /= np.hypot(*x)\n",
        "    x *= max(np.hypot(*eye_to_eye) * 2.0, np.hypot(*eye_to_mouth) * 1.8)\n",
        "    y = np.flipud(x) * [-1, 1]\n",
        "    c = eye_avg + eye_to_mouth * 0.1\n",
        "    quad = np.stack([c - x - y, c - x + y, c + x + y, c + x - y])\n",
        "    qsize = np.hypot(*x) * 2\n",
        "\n",
        "    # read image\n",
        "    # #img = PIL.Image.open(filepath)\n",
        "    # #img = PIL.Image.fromarray(img)\n",
        "\n",
        "    # -------------------------------------------\n",
        "    # -------------------------------------------\n",
        "    hei,wid = img.size\n",
        "    output_size = hei\n",
        "    transform_size = hei\n",
        "    enable_padding = True\n",
        "    # -------------------------------------------\n",
        "    # -------------------------------------------\n",
        "    # Shrink.\n",
        "    shrink = int(np.floor(qsize / output_size * 0.5))\n",
        "    if shrink > 1:\n",
        "        rsize = (int(np.rint(float(img.size[0]) / shrink)), int(np.rint(float(img.size[1]) / shrink)))\n",
        "        img = img.resize(rsize, PIL.Image.ANTIALIAS)\n",
        "        quad /= shrink\n",
        "        qsize /= shrink\n",
        "\n",
        "    # Crop.\n",
        "    border = max(int(np.rint(qsize * 0.1)), 3)\n",
        "    crop = (int(np.floor(min(quad[:, 0]))), int(np.floor(min(quad[:, 1]))), int(np.ceil(max(quad[:, 0]))),\n",
        "            int(np.ceil(max(quad[:, 1]))))\n",
        "    crop = (max(crop[0] - border, 0), max(crop[1] - border, 0), min(crop[2] + border, img.size[0]),\n",
        "            min(crop[3] + border, img.size[1]))\n",
        "    if crop[2] - crop[0] < img.size[0] or crop[3] - crop[1] < img.size[1]:\n",
        "        img = img.crop(crop)\n",
        "        quad -= crop[0:2]\n",
        "\n",
        "    # Pad.\n",
        "    pad = (int(np.floor(min(quad[:, 0]))), int(np.floor(min(quad[:, 1]))), int(np.ceil(max(quad[:, 0]))),\n",
        "           int(np.ceil(max(quad[:, 1]))))\n",
        "    pad = (max(-pad[0] + border, 0), max(-pad[1] + border, 0), max(pad[2] - img.size[0] + border, 0),\n",
        "           max(pad[3] - img.size[1] + border, 0))\n",
        "    if enable_padding and max(pad) > border - 4:\n",
        "        pad = np.maximum(pad, int(np.rint(qsize * 0.3)))\n",
        "        img = np.pad(np.float32(img), ((pad[1], pad[3]), (pad[0], pad[2]), (0, 0)), 'reflect')\n",
        "        h, w, _ = img.shape\n",
        "        y, x, _ = np.ogrid[:h, :w, :1]\n",
        "        mask = np.maximum(1.0 - np.minimum(np.float32(x) / pad[0], np.float32(w - 1 - x) / pad[2]),\n",
        "                          1.0 - np.minimum(np.float32(y) / pad[1], np.float32(h - 1 - y) / pad[3]))\n",
        "        blur = qsize * 0.02\n",
        "        img += (scipy.ndimage.gaussian_filter(img, [blur, blur, 0]) - img) * np.clip(mask * 3.0 + 1.0, 0.0, 1.0)\n",
        "        img += (np.median(img, axis=(0, 1)) - img) * np.clip(mask, 0.0, 1.0)\n",
        "        img = PIL.Image.fromarray(np.uint8(np.clip(np.rint(img), 0, 255)), 'RGB')\n",
        "        quad += pad[:2]\n",
        "\n",
        "    # Transform.\n",
        "    img = img.transform((transform_size, transform_size), PIL.Image.QUAD, (quad + 0.5).flatten(), PIL.Image.BILINEAR)\n",
        "    if output_size < transform_size:\n",
        "        img = img.resize((output_size, output_size), PIL.Image.ANTIALIAS)\n",
        "\n",
        "    # Return aligned image.\n",
        "    return img\n",
        "\n",
        "def run_alignment(image, predictor, detector):\n",
        "  aligned_image = align_face(image, predictor=predictor, detector=detector) \n",
        "  print(\"Aligned image has shape: {}\".format(aligned_image.size))\n",
        "  return aligned_image \n",
        "  "
      ],
      "metadata": {
        "id": "I_ItOAYysQQC"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Misc utils\n"
      ],
      "metadata": {
        "id": "ZY3nuZFVuDSW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def display_alongside_source_image(result_image, source_image):\n",
        "    res = np.concatenate([np.array(source_image.resize(resize_dims)),\n",
        "                          np.array(result_image.resize(resize_dims))], axis=1)\n",
        "    return Image.fromarray(res)\n",
        "\n",
        "def create_video_1(img_array, vid_path='interp_video.avi'):\n",
        "    import cv2\n",
        "    size = np.shape(img_array[0])[:2]\n",
        "    out = cv2.VideoWriter(vid_path,cv2.VideoWriter_fourcc(*'DIVX'), 10, size)\n",
        "    \n",
        "    for i in range(len(img_array)):\n",
        "        out.write(cv2.cvtColor(np.array(img_array[i]),cv2.COLOR_RGB2BGR))\n",
        "    out.release()\n",
        "\n",
        "def create_video(img_array, vid_path='interp_video.avi'):\n",
        "    import cv2\n",
        "    size = np.shape(img_array[0][0])[:2]\n",
        "    out = cv2.VideoWriter(vid_path,cv2.VideoWriter_fourcc(*'DIVX'), 10, size)\n",
        "    for j in range(len(img_array)):\n",
        "        for i in range(len(img_array[j])):\n",
        "            out.write(cv2.cvtColor(np.array(img_array[j][i]),cv2.COLOR_RGB2BGR))\n",
        "            # #cv2_imshow(cv2.cvtColor(np.array(img_array[j][i]),cv2.COLOR_RGB2BGR))\n",
        "            cv2.waitKey(50)\n",
        "    \n",
        "    out.release()\n"
      ],
      "metadata": {
        "id": "vZtvCgDjuIV_"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## HFGI Utils"
      ],
      "metadata": {
        "id": "uZzgxHfUp3Bk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_latents(net, x, is_cars=False):\n",
        "    codes = net.encoder(x)\n",
        "    if net.opts.start_from_latent_avg:\n",
        "        if codes.ndim == 2:\n",
        "            codes = codes + net.latent_avg.repeat(codes.shape[0], 1, 1)[:, 0, :]\n",
        "        else:\n",
        "            codes = codes + net.latent_avg.repeat(codes.shape[0], 1, 1)\n",
        "    if codes.shape[1] == 18 and is_cars:\n",
        "        codes = codes[:, :16, :]\n",
        "    return codes\n",
        "\n",
        "# Setup required image transformations\n",
        "transform_img = transforms.Compose([\n",
        "    transforms.Resize((256, 256)),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize([0.5, 0.5, 0.5], [0.5, 0.5, 0.5])])\n",
        "resize_dims = (256, 256)\n",
        "\n",
        "def load_hfgi_model(model_path):\n",
        "    ckpt = torch.load(model_path, map_location='cpu')\n",
        "    opts = ckpt['opts']\n",
        "    opts['is_train'] = False\n",
        "    opts['checkpoint_path'] = model_path\n",
        "    opts= Namespace(**opts)\n",
        "    net = pSp(opts)\n",
        "    net.eval()\n",
        "    net.cuda()\n",
        "    print('Model successfully loaded!')\n",
        "    return net\n",
        "\n",
        "\n",
        "\"\"\"\n",
        "The function takes a preprocessed aligend and transcfomed image tensor and convert it into its latent vector\n",
        "Using the pretrained HFGI model\n",
        "Then returns an image along with its latent vector\n",
        "\"\"\"\n",
        "def img_2_latent_proj(transformed_img, net):\n",
        "    with torch.no_grad():\n",
        "        x = transformed_img.unsqueeze(0).cuda()\n",
        "\n",
        "        # tic = time.time()\n",
        "        latent_codes = get_latents(net, x)\n",
        "        \n",
        "        # calculate the distortion map\n",
        "        imgs, _ = net.decoder([latent_codes[0].unsqueeze(0).cuda()],None, input_is_latent=True, randomize_noise=False, return_latents=True)\n",
        "        res = x -  torch.nn.functional.interpolate(torch.clamp(imgs, -1., 1.), size=(256,256) , mode='bilinear')\n",
        "\n",
        "        # ADA\n",
        "        img_edit = torch.nn.functional.interpolate(torch.clamp(imgs, -1., 1.), size=(256,256) , mode='bilinear')\n",
        "        res_align  = net.grid_align(torch.cat((res, img_edit  ), 1))\n",
        "\n",
        "        # consultation fusion\n",
        "        conditions = net.residue(res_align)\n",
        "\n",
        "        result_image, lat = net.decoder([latent_codes],conditions, input_is_latent=True, randomize_noise=False, return_latents=True)\n",
        "        # toc = time.time()\n",
        "        # print('Inference took {:.4f} seconds.'.format(toc - tic))\n",
        "    \n",
        "    return tensor2im(result_image[0]), latent_codes\n",
        "\n",
        "def process_with_hfgi(img, net, predictor, detector):\n",
        "    # #img = img.resize((256,256))\n",
        "    aligned_img = align_face(img, predictor, detector)\n",
        "    transformed_img = transform_img(aligned_img)\n",
        "    projected_img, latent_vec = img_2_latent_proj(transformed_img, net)\n",
        "\n",
        "    return aligned_img, transformed_img, projected_img, latent_vec\n"
      ],
      "metadata": {
        "id": "q2j36KEKrDhH"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Load Models"
      ],
      "metadata": {
        "id": "M5X-wfdzthJY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import dlib\n",
        "\n",
        "# Load the models\n",
        "predictor = dlib.shape_predictor(\"HFGI/checkpoint/shape_predictor_68_face_landmarks.dat\")\n",
        "detector = dlib.get_frontal_face_detector()\n",
        "net = load_hfgi_model('HFGI/checkpoint/ckpt.pt')"
      ],
      "metadata": {
        "id": "wUWvyMR-rDcC",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9cddd95c-2ca6-4295-da4a-cfa02aaeb1db"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading basic encoder from checkpoint: HFGI/checkpoint/ckpt.pt\n",
            "Model successfully loaded!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# HFGI Editing"
      ],
      "metadata": {
        "id": "qCXp5Dhmzh37"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## interface gan"
      ],
      "metadata": {
        "id": "-IbBveLPaFk1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def project_change_inferfacegan(transformed_img, net, edit_direction, edit_degree):\n",
        "    with torch.no_grad():\n",
        "        x = transformed_img.unsqueeze(0).cuda()\n",
        "\n",
        "        # tic = time.time()\n",
        "        latent_codes = get_latents(net, x)\n",
        "        \n",
        "        # calculate the distortion map\n",
        "        imgs, _ = net.decoder([latent_codes[0].unsqueeze(0).cuda()],None, input_is_latent=True, randomize_noise=False, return_latents=True)\n",
        "        res = x -  torch.nn.functional.interpolate(torch.clamp(imgs, -1., 1.), size=(256,256) , mode='bilinear')\n",
        "\n",
        "        # ADA\n",
        "        img_edit = torch.nn.functional.interpolate(torch.clamp(imgs, -1., 1.), size=(256,256) , mode='bilinear')\n",
        "        res_align  = net.grid_align(torch.cat((res, img_edit  ), 1))\n",
        "\n",
        "        # consultation fusion\n",
        "        conditions = net.residue(res_align)\n",
        "\n",
        "        result_image, img_latent = net.decoder([latent_codes],conditions, input_is_latent=True, randomize_noise=False, return_latents=True)\n",
        "        # toc = time.time()\n",
        "        # print('Inference took {:.4f} seconds.'.format(toc - tic))\n",
        "\n",
        "    img_edit, edit_latents = editor.apply_interfacegan(latent_codes[0].unsqueeze(0).cuda(), edit_direction, factor=edit_degree)\n",
        "    # align the distortion map\n",
        "    img_edit = torch.nn.functional.interpolate(torch.clamp(img_edit, -1., 1.), size=(256,256) , mode='bilinear')\n",
        "    res_align  = net.grid_align(torch.cat((res, img_edit  ), 1))\n",
        "\n",
        "    # fusion\n",
        "    conditions = net.residue(res_align)\n",
        "    result, _ = net.decoder([edit_latents],conditions, input_is_latent=True, randomize_noise=False, return_latents=True)\n",
        "\n",
        "    result = torch.nn.functional.interpolate(result, size=(256,256) , mode='bilinear')\n",
        "    \n",
        "\n",
        "    return result, edit_latents, latent_codes\n",
        "\n",
        "def manipulate_one_inter(transformed_img, net, dir_torch):\n",
        "\n",
        "    edit_degree_max = 5\n",
        "    edit_degree_min = -5\n",
        "    num_steps = 60\n",
        "\n",
        "    new_steps = num_steps-int(num_steps/2)\n",
        "\n",
        "    step_size = (edit_degree_max-edit_degree_min)/new_steps\n",
        "    edit_degree = 0\n",
        "\n",
        "    interp_images = []\n",
        "    for i in range(0,num_steps+1):\n",
        "        # result, result_latent = project_change_inferfacegan(transformed_img, net, 2*dir_torch+age_direction, edit_degree)\n",
        "        result, result_latent, org_latent  = project_change_inferfacegan(transformed_img, net, dir_torch, edit_degree)\n",
        "        if (i>=0 and i<int(num_steps/4)):\n",
        "            edit_degree = edit_degree+step_size\n",
        "            interp_images.append(np.array(tensor2im(result[0])))\n",
        "        elif (i>=int(num_steps/4) and i<int(num_steps*(3/4))):\n",
        "            edit_degree = edit_degree-step_size\n",
        "            interp_images.append(np.array(tensor2im(result[0])))\n",
        "        else:\n",
        "            edit_degree = edit_degree+step_size\n",
        "            interp_images.append(np.array(tensor2im(result[0])))\n",
        "\n",
        "    return interp_images\n",
        "\n",
        "import random\n",
        "\n",
        "def make_random_video_inter(transformed_img, net, all_directions, tot_directions=10):\n",
        "    \n",
        "    all_interps = []\n",
        "    for i in range(0,tot_directions):\n",
        "        direction_path = random.choice(all_directions)\n",
        "        dir_torch = load_direction(direction_path)\n",
        "        # #edit_degree = 1.5\n",
        "        print(direction_path.split('/')[-1])\n",
        "        # print(dir_torch.max())\n",
        "\n",
        "        if dir_torch.max()<0.1:\n",
        "            scale_direction = 0.14/dir_torch.max()\n",
        "            dir_torch = dir_torch*scale_direction\n",
        "\n",
        "        # print(dir_torch.max())\n",
        "\n",
        "        interp_images = manipulate_one_inter(transformed_img, net, dir_torch)\n",
        "        all_interps.append(interp_images)\n",
        "\n",
        "    return all_interps\n",
        "\n",
        "def load_direction(direction_path):\n",
        "    dir_numpy = np.load(direction_path)\n",
        "    dir_torch = torch.from_numpy(dir_numpy).float().cuda()\n",
        "\n",
        "    dir_torch = dir_torch[0].unsqueeze(0)\n",
        "    return dir_torch\n"
      ],
      "metadata": {
        "id": "Nl1nffiz0j7i"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# from google.colab.patches import cv2_imshow\n",
        "# import cv2\n",
        "\n",
        "# img_path = '/content/CHARACTER 010008.png'\n",
        "# img = PIL.Image.open(img_path).convert('RGB')\n",
        "\n",
        "# aligned_img, transformed_img, projected_img, latent_vec = process_with_hfgi(img, net, predictor, detector)\n",
        "\n",
        "# # comb = display_alongside_source_image(projected_img, aligned_img)\n",
        "# # cv2_imshow(cv2.cvtColor(np.array(comb), cv2.COLOR_RGB2BGR))"
      ],
      "metadata": {
        "id": "CR_VNr6rlKgD"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# from glob import glob\n",
        "# from editings import latent_editor\n",
        "# editor = latent_editor.LatentEditor(net.decoder)\n",
        "\n",
        "# # all_directions = glob('/content/interfacegan/boundaries/stylegan_ffhq_*')\n",
        "# all_directions = glob('/content/stylegan2-distillation/stylegan2directions/*.npy')\n",
        "\n",
        "# direction_path = all_directions[-5] \n",
        "# print(direction_path)\n",
        "# dir_torch = load_direction(direction_path)\n",
        "# edit_degree = 1.5"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_IbvWsrfij-k",
        "outputId": "b4fe3717-a1b5-4736-b85a-7a1626969f7c"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/stylegan2-distillation/stylegan2directions/smile.npy\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# aligned_img, transformed_img, projected_img, latent_vec = process_with_hfgi(img, net, predictor, detector)\n",
        "# result, result_latent, orig_latent = project_change_inferfacegan(transformed_img, net, dir_torch, edit_degree)\n",
        "# comb = display_alongside_source_image(tensor2im(result[0]), aligned_img)\n",
        "# cv2_imshow(cv2.cvtColor(np.array(comb), cv2.COLOR_RGB2BGR))"
      ],
      "metadata": {
        "id": "QjvwAXApiao-"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# all_interps = make_random_video_inter(transformed_img, net, all_directions, tot_directions=3)\n",
        "# create_video(all_interps, vid_path='smooth.avi')"
      ],
      "metadata": {
        "id": "kHlz_Lvq4iux"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## gan space"
      ],
      "metadata": {
        "id": "vDSJNM5qLhKF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "def project_change_ganspace(transformed_img, net, edit_direction, edit_degree, ganspace_pca):\n",
        "    with torch.no_grad():\n",
        "        x = transformed_img.unsqueeze(0).cuda()\n",
        "\n",
        "        # tic = time.time()\n",
        "        latent_codes = get_latents(net, x)\n",
        "        \n",
        "        # calculate the distortion map\n",
        "        imgs, _ = net.decoder([latent_codes[0].unsqueeze(0).cuda()],None, input_is_latent=True, randomize_noise=False, return_latents=True)\n",
        "        res = x -  torch.nn.functional.interpolate(torch.clamp(imgs, -1., 1.), size=(256,256) , mode='bilinear')\n",
        "\n",
        "        # ADA\n",
        "        img_edit = torch.nn.functional.interpolate(torch.clamp(imgs, -1., 1.), size=(256,256) , mode='bilinear')\n",
        "        res_align  = net.grid_align(torch.cat((res, img_edit  ), 1))\n",
        "\n",
        "        # consultation fusion\n",
        "        conditions = net.residue(res_align)\n",
        "\n",
        "        result_image, img_latent = net.decoder([latent_codes],conditions, input_is_latent=True, randomize_noise=False, return_latents=True)\n",
        "        # toc = time.time()\n",
        "        # print('Inference took {:.4f} seconds.'.format(toc - tic))\n",
        "\n",
        "    \n",
        "    edit_direction = (edit_direction[0],edit_direction[1],edit_direction[2],edit_degree)\n",
        "\n",
        "    img_edit, edit_latents = editor.apply_ganspace(latent_codes[0].unsqueeze(0).cuda(), ganspace_pca, [edit_direction])\n",
        "    # align the distortion map\n",
        "    img_edit = torch.nn.functional.interpolate(torch.clamp(img_edit, -1., 1.), size=(256,256) , mode='bilinear')\n",
        "    res_align  = net.grid_align(torch.cat((res, img_edit  ), 1))\n",
        "    conditions = net.residue(res_align)\n",
        "    result, _ = net.decoder([edit_latents],conditions, input_is_latent=True, randomize_noise=False, return_latents=True)\n",
        "    result = torch.nn.functional.interpolate(result, size=(256,256) , mode='bilinear')\n",
        "\n",
        "    return result, edit_latents, latent_codes\n",
        "\n",
        "def manipulate_one_ganspace(transformed_img, net, edit_direction, ganspace_pca):\n",
        "    \n",
        "    edit_degree_max = 5*4\n",
        "    edit_degree_min = -5*4\n",
        "    num_steps = 60\n",
        "\n",
        "    new_steps = num_steps-int(num_steps/2)\n",
        "\n",
        "    step_size = (edit_degree_max-edit_degree_min)/new_steps\n",
        "    edit_degree = 0\n",
        "\n",
        "    interp_images = []\n",
        "    for i in range(0,num_steps+1):\n",
        "        # print(edit_degree)\n",
        "        result, result_latent, org_latent = project_change_ganspace(transformed_img, net, edit_direction, edit_degree, ganspace_pca)\n",
        "\n",
        "        if (i>=0 and i<int(num_steps/4)):\n",
        "            edit_degree = edit_degree+step_size\n",
        "            interp_images.append(np.array(tensor2im(result[0])))\n",
        "        elif (i>=int(num_steps/4) and i<int(num_steps*(3/4))):\n",
        "            edit_degree = edit_degree-step_size\n",
        "            interp_images.append(np.array(tensor2im(result[0])))\n",
        "        else:\n",
        "            edit_degree = edit_degree+step_size\n",
        "            interp_images.append(np.array(tensor2im(result[0])))\n",
        "\n",
        "    return interp_images\n",
        "\n",
        "import random\n",
        "\n",
        "def make_random_video_ganspace(transformed_img, net, ganspace_directions, ganspace_pca, tot_directions=3):\n",
        "    all_interps = []\n",
        "    all_directions = list(ganspace_directions.keys())\n",
        "    \n",
        "    for i in range(0,tot_directions):\n",
        "        direction_path = random.choice(all_directions)\n",
        "        print(direction_path)\n",
        "        edit_direction = ganspace_directions[direction_path]\n",
        "\n",
        "        # print(dir_torch.max())\n",
        "        interp_images = manipulate_one_ganspace(transformed_img, net, edit_direction, ganspace_pca)\n",
        "        all_interps.append(interp_images)\n",
        "\n",
        "    return all_interps"
      ],
      "metadata": {
        "id": "O9pcZULkJBIK"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# (idx, edit_start, edit_end, strength, invert)\n",
        "ganspace_directions = {\n",
        "    # StyleGAN2 cars W\n",
        "    # 'Redness':          (22,  9, 11,   -8, False),\n",
        "    # 'Horizontal flip':  ( 0,  0,  5,  2.0, True),\n",
        "    # 'Add grass':        (41,  9, 11,  -18, False),\n",
        "    # 'Blocky shape':     (16,  3,  6,   25, False),\n",
        "\n",
        "    # # BigGAN-512 irish_setter\n",
        "    # 'Move right':       ( 0,  0, 15, -1.5, False),\n",
        "    # 'Rotate':           ( 3,  0, 15,  -0.5, False),\n",
        "    # 'Move back':        ( 4,  0, 15,  2.5, False),\n",
        "    # 'Zoom in':          ( 6,  0, 15, -2.0, False),\n",
        "    # 'Zoom out':         (12,  0, 15, -4.0, False),\n",
        "    # 'Sharpen BG':       (13,  6,  9, 20.0, False),\n",
        "    # 'Camera down':      (15,  1,  6, -4.0, False),\n",
        "    # 'Light right':      (28,  7,  8,  30, False),\n",
        "    # 'Pixelate':         (46, 10, 11,  -25, False),\n",
        "    # 'Reeds':            (61,  4,  8,  -15, False),\n",
        "    # 'Dry BG':           (65,  6,  8,  -30, False),\n",
        "    # 'Grass length':     (69,  5,  8,   15, False),\n",
        "\n",
        "    # StyleGAN2 ffhq\n",
        "    'frizzy_hair':             (31,  2,  6,  20, False),\n",
        "    'background_blur':         (49,  6,  9,  20, False),\n",
        "    'bald':                    (21,  2,  5,  20, False),\n",
        "    'big_smile':               (19,  4,  5,  20, False),\n",
        "    'caricature_smile':        (26,  3,  8,  13, False),\n",
        "    'scary_eyes':              (33,  6,  8,  20, False),\n",
        "    'curly_hair':              (47,  3,  6,  20, False),\n",
        "    'dark_bg_shiny_hair':      (13,  8,  9,  20, False),\n",
        "    'dark_hair_and_light_pos': (14,  8,  9,  20, False),\n",
        "    'dark_hair':               (16,  8,  9,  20, False),\n",
        "    'disgusted':               (43,  6,  8, -30, False),\n",
        "    'displeased':              (36,  4,  7,  20, False),\n",
        "    'eye_openness':            (54,  7,  8,  20, False),\n",
        "    'eye_wrinkles':            (28,  6,  8,  20, False),\n",
        "    'eyebrow_thickness':       (37,  8,  9,  20, False),\n",
        "    'face_roundness':          (37,  0,  5,  20, False),\n",
        "    'fearful_eyes':            (54,  4, 10,  20, False),\n",
        "    'hairline':                (21,  4,  5, -20, False),\n",
        "    'happy_frizzy_hair':       (30,  0,  8,  20, False),\n",
        "    'happy_elderly_lady':      (27,  4,  7,  20, False),\n",
        "    'head_angle_up':           (11,  1,  4,  20, False),\n",
        "    'huge_grin':               (28,  4,  6,  20, False),\n",
        "    'in_awe':                  (23,  3,  6, -15, False),\n",
        "    'wide_smile':              (23,  3,  6,  20, False),\n",
        "    'large_jaw':               (22,  3,  6,  20, False),\n",
        "    'light_lr':                (15,  8,  9,  10, False),\n",
        "    'lipstick_and_age':        (34,  6, 11,  20, False),\n",
        "    'lipstick':                (34, 10, 11,  20, False),\n",
        "    'mascara_vs_beard':        (41,  6,  9,  20, False),\n",
        "    'nose_length':             (51,  4,  5, -20, False),\n",
        "    'elderly_woman':           (34,  6,  7,  20, False),\n",
        "    'overexposed':             (27,  8, 18,  15, False),\n",
        "    'screaming':               (35,  3,  7, -15, False),\n",
        "    'short_face':              (32,  2,  6, -20, False),\n",
        "    'show_front_teeth':        (59,  4,  5,  40, False),\n",
        "    'smile':                   (46,  4,  5, -20, False),\n",
        "    'straight_bowl_cut':       (20,  4,  5, -20, False),\n",
        "    'sunlight_in_face':        (10,  8,  9,  10, False),\n",
        "    'trimmed_beard':           (58,  7,  9,  20, False),\n",
        "    'white_hair':              (57,  7, 10, -24, False),\n",
        "    'wrinkles':                (20,  6,  7, -18, False),\n",
        "    'boyishness':              (8,   2,  5,  20, False),\n",
        "}"
      ],
      "metadata": {
        "id": "qcoNGGPyiq_R"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ganspace_pca = torch.load('HFGI/editings/ganspace_pca/ffhq_pca.pt') \n",
        "# edit_degree = 20\n",
        "# edit_direction = ganspace_directions['lipstick']\n",
        "\n",
        "# result, result_latent, org_latent = project_change_ganspace(transformed_img, net, edit_direction, edit_degree, ganspace_pca)\n",
        "# comb = display_alongside_source_image(tensor2im(result[0]), aligned_img)\n",
        "# cv2_imshow(cv2.cvtColor(np.array(comb), cv2.COLOR_RGB2BGR))"
      ],
      "metadata": {
        "id": "z14ENr_PiWxn"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# interp_images = manipulate_one_ganspace(transformed_img, net, edit_direction, ganspace_pca)\n",
        "# create_video_1(interp_images,vid_path='gany_2.avi')"
      ],
      "metadata": {
        "id": "M39nSho8iT85"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# interp_images = make_random_video_ganspace(transformed_img, net, ganspace_directions, ganspace_pca, tot_directions=3)\n",
        "# create_video(interp_images,vid_path='gany.avi')"
      ],
      "metadata": {
        "id": "w8hWLmCSScbR"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Mix both"
      ],
      "metadata": {
        "id": "_NJc5cQe60He"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def make_random_video_mix(transformed_img, net, all_directions, ganspace_directions, ganspace_pca, tot_directions=2):\n",
        "    all_interps_inter = make_random_video_inter(transformed_img, net, all_directions, tot_directions=tot_directions)\n",
        "    all_interps_ganspace = make_random_video_ganspace(transformed_img, net, ganspace_directions, ganspace_pca, tot_directions=tot_directions)\n",
        "\n",
        "    all_images_mix = []\n",
        "    for i in range(0,tot_directions):\n",
        "        all_images_mix.append(all_interps_inter[i])\n",
        "        all_images_mix.append(all_interps_ganspace[i])\n",
        "\n",
        "    return all_images_mix"
      ],
      "metadata": {
        "id": "ShVInUQcnriS"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from glob import glob\n",
        "from editings import latent_editor\n",
        "editor = latent_editor.LatentEditor(net.decoder)\n",
        "\n",
        "# all_directions = glob('/content/interfacegan/boundaries/stylegan_ffhq_*')\n",
        "all_directions = glob('/content/stylegan2-distillation/stylegan2directions/*.npy')\n",
        "\n",
        "# direction_path = all_directions[-5] \n",
        "# print(direction_path)\n",
        "# dir_torch = load_direction(direction_path)\n",
        "# edit_degree = 1.5\n",
        "\n",
        "ganspace_pca = torch.load('HFGI/editings/ganspace_pca/ffhq_pca.pt') \n",
        "\n",
        "# set the path of the image and paste the path here\n",
        "img_path = '/content/character gan010011.png'\n",
        "img = PIL.Image.open(img_path).convert('RGB')\n",
        "\n",
        "# sets the size of the video\n",
        "tot_directions = 4\n",
        "\n",
        "aligned_img, transformed_img, projected_img, latent_vec = process_with_hfgi(img, net, predictor, detector)\n",
        "all_images_mix = make_random_video_mix(transformed_img, net, all_directions, ganspace_directions, ganspace_pca, tot_directions=tot_directions)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5hxG9_AZpe3Z",
        "outputId": "c7a74437-d39d-4eca-c5ba-5bf0bf16312a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "yaw.npy\n",
            "pitch.npy\n",
            "eyes_open.npy\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "create_video(all_images_mix, vid_path='interpolation.avi')"
      ],
      "metadata": {
        "id": "L0J5dCiT5gC7"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "vJCZPJ9t6Ddt"
      },
      "execution_count": 20,
      "outputs": []
    }
  ]
}